name: Oracle System Health & Monitoring
# Monitors the health of DeepSeek, Jules, and Mem0 AI services
# Runs hourly health checks and generates daily reports

on:
  schedule:
    # Health check every 1 hour
    - cron: '0 * * * *'
    # Daily report at 06:00 UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      check_type:
        description: 'Type of health check'
        required: true
        default: 'full'
        type: choice
        options:
          - quick    # 5min - basic endpoints
          - full     # 30min - full diagnostic
          - deep     # 2hr - stress test

concurrency:
  group: oracle-monitoring
  cancel-in-progress: false

permissions:
  contents: read
  actions: read
  issues: write
  statuses: write

env:
  HEALTH_CHECK_TIMEOUT_MS: 10000
  # SECURITY: Never log API keys directly. All secrets are masked in outputs.
  METRICS_RETENTION_DAYS: 30

jobs:
  # ========================================
  # PHASE 1: Quick Health Checks
  # ========================================
  quick-health-check:
    name: Quick Health Check (5 min)
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' ||
      github.event.inputs.check_type == 'quick' ||
      github.event.inputs.check_type == 'full' ||
      github.event.inputs.check_type == 'deep'
    
    outputs:
      deepseek_status: ${{ steps.deepseek.outputs.status }}
      deepseek_time_ms: ${{ steps.deepseek.outputs.response_time_ms }}
      jules_status: ${{ steps.jules.outputs.status }}
      jules_time_ms: ${{ steps.jules.outputs.response_time_ms }}
      mem0_status: ${{ steps.mem0.outputs.status }}
      mem0_time_ms: ${{ steps.mem0.outputs.response_time_ms }}
      check_timestamp: ${{ steps.timestamp.outputs.iso }}
      all_healthy: ${{ steps.summary.outputs.all_healthy }}

    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: pnpm

      - name: Get current timestamp
        id: timestamp
        run: |
          echo "iso=$(date -u +'%Y-%m-%dT%H:%M:%SZ')" >> "$GITHUB_OUTPUT"
          echo "unix=$(date +%s)" >> "$GITHUB_OUTPUT"

      - name: Validate API Keys (Secrets Safety)
        run: |
          if [ -z "${{ secrets.DEEPSEEK_API_KEY }}" ]; then
            echo "ERROR: DEEPSEEK_API_KEY is empty"
            exit 1
          fi
          if [ -z "${{ secrets.DEEPSEEK_BASE_URL }}" ]; then
            echo "ERROR: DEEPSEEK_BASE_URL is empty"
            exit 1
          fi
          echo "‚úÖ API Keys validation passed (keys verified, not logged)"
          
      - name: Check DeepSeek Health

        id: deepseek
        continue-on-error: true
        run: |
          set +e
          START_TIME=$(date +%s%N)
          
          RESPONSE=$(curl -s -w "\n%{http_code}" \
            -H "Authorization: Bearer ${{ secrets.DEEPSEEK_API_KEY }}" \
            -H "${{ secrets.NSCALE_HEADER_NAME }}: ${{ secrets.NSCALE_API_KEY }}" \
            --max-time 10 \
            --connect-timeout 5 \
            "${{ secrets.DEEPSEEK_BASE_URL }}/v1/models" \
            2>&1)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | head -n-1)
          END_TIME=$(date +%s%N)
          RESPONSE_TIME_MS=$(( (END_TIME - START_TIME) / 1000000 ))
          
          if [ "$HTTP_CODE" = "200" ]; then
            STATUS="healthy"
            MASKED_KEY="${{ secrets.DEEPSEEK_API_KEY:0:10 }}***"
echo "‚úÖ DeepSeek: $RESPONSE_TIME_MS ms (API key starts with $MASKED_KEY)"
          elif [ "$HTTP_CODE" = "429" ]; then
            STATUS="rate_limited"
            echo "‚ö†Ô∏è DeepSeek: Rate limited (429)"
          elif [ -z "$HTTP_CODE" ]; then
            STATUS="unreachable"
            echo "‚ùå DeepSeek: Connection timeout"
          else
            STATUS="error"
            echo "‚ùå DeepSeek: HTTP $HTTP_CODE" # No secrets in error message
          fi
          
          echo "status=$STATUS" >> "$GITHUB_OUTPUT"
          echo "response_time_ms=$RESPONSE_TIME_MS" >> "$GITHUB_OUTPUT"
          echo "http_code=$HTTP_CODE" >> "$GITHUB_OUTPUT"

      - name: Check Jules Health
        id: jules
        continue-on-error: true
        run: |
          set +e
          START_TIME=$(date +%s%N)
          
          RESPONSE=$(curl -s -w "\n%{http_code}" \
            -H "Authorization: Bearer ${{ secrets.JULES_API_KEY }}" \
            --max-time 10 \
            --connect-timeout 5 \
            "${{ secrets.JULES_BASE_URL }}/health" \
            2>&1)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | head -n-1)
          END_TIME=$(date +%s%N)
          RESPONSE_TIME_MS=$(( (END_TIME - START_TIME) / 1000000 ))
          
          if [ "$HTTP_CODE" = "200" ]; then
            # Parse health response
            if echo "$BODY" | grep -q '"status":"ok"'; then
              STATUS="healthy"
            else
              STATUS="degraded"
            fi
            echo "‚úÖ Jules: $RESPONSE_TIME_MS ms"
          elif [ "$HTTP_CODE" = "429" ]; then
            STATUS="rate_limited"
            echo "‚ö†Ô∏è Jules: Rate limited (429)"
          elif [ -z "$HTTP_CODE" ]; then
            STATUS="unreachable"
            echo "‚ùå Jules: Connection timeout"
          else
            STATUS="error"
            echo "‚ùå Jules: HTTP $HTTP_CODE" # No secrets in error message
          fi
          
          echo "status=$STATUS" >> "$GITHUB_OUTPUT"
          echo "response_time_ms=$RESPONSE_TIME_MS" >> "$GITHUB_OUTPUT"
          echo "http_code=$HTTP_CODE" >> "$GITHUB_OUTPUT"

      - name: Check Mem0 Health
        id: mem0
        continue-on-error: true
        run: |
          set +e
          START_TIME=$(date +%s%N)
          
          RESPONSE=$(curl -s -w "\n%{http_code}" \
            --max-time 10 \
            --connect-timeout 5 \
            "${{ secrets.MCP_SERVER_URL }}/health" \
            2>&1)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | head -n-1)
          END_TIME=$(date +%s%N)
          RESPONSE_TIME_MS=$(( (END_TIME - START_TIME) / 1000000 ))
          
          if [ "$HTTP_CODE" = "200" ]; then
            STATUS="healthy"
            echo "‚úÖ Mem0: $RESPONSE_TIME_MS ms"
          elif [ -z "$HTTP_CODE" ]; then
            STATUS="unreachable"
            echo "‚ùå Mem0: Connection timeout"
          else
            STATUS="error"
            echo "‚ùå Mem0: HTTP $HTTP_CODE" # No secrets in error message
          fi
          
          echo "status=$STATUS" >> "$GITHUB_OUTPUT"
          echo "response_time_ms=$RESPONSE_TIME_MS" >> "$GITHUB_OUTPUT"
          echo "http_code=$HTTP_CODE" >> "$GITHUB_OUTPUT"

      - name: Summarize Health Status
        id: summary
        run: |
          DEEPSEEK="${{ steps.deepseek.outputs.status }}"
          JULES="${{ steps.jules.outputs.status }}"
          MEM0="${{ steps.mem0.outputs.status }}"
          
          if [[ "$DEEPSEEK" == "healthy" && "$JULES" == "healthy" && "$MEM0" == "healthy" ]]; then
            ALL_HEALTHY="true"
            EMOJI="‚úÖ"
          else
            ALL_HEALTHY="false"
            EMOJI="‚ö†Ô∏è"
          fi
          
          echo "all_healthy=$ALL_HEALTHY" >> "$GITHUB_OUTPUT"
          echo "${EMOJI} Oracle Health Status at ${{ steps.timestamp.outputs.iso }}" >> "$GITHUB_STEP_SUMMARY"
          echo "| Service | Status | Response Time |" >> "$GITHUB_STEP_SUMMARY"
          echo "|---------|--------|--------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| DeepSeek | $DEEPSEEK | ${{ steps.deepseek.outputs.response_time_ms }}ms |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Jules | $JULES | ${{ steps.jules.outputs.response_time_ms }}ms |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Mem0 | $MEM0 | ${{ steps.mem0.outputs.response_time_ms }}ms |" >> "$GITHUB_STEP_SUMMARY"

      - name: Store metrics in cache
        if: always()
        uses: actions/cache/save@v3
        with:
          path: oracle-metrics.json
          key: oracle-metrics-${{ github.run_id }}

  # ========================================
  # PHASE 2: Detailed Diagnostics (Optional)
  # ========================================
  detailed-diagnostics:
    name: Detailed Diagnostics (30 min)
    runs-on: ubuntu-latest
    needs: quick-health-check
    if: |
      (github.event.inputs.check_type == 'full' ||
       github.event.inputs.check_type == 'deep') ||
      needs.quick-health-check.outputs.all_healthy == 'false'
    
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: pnpm

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run DeepSeek diagnostic
        id: deepseek-diag
        continue-on-error: true
        run: |
          npx ts-node scripts/oracle/diagnostic-deepseek.ts \
            --service-url "${{ secrets.DEEPSEEK_BASE_URL }}" \
            --api-key "${{ secrets.DEEPSEEK_API_KEY }}" \
            --timeout 30000
        env:
          NSCALE_HEADER_NAME: ${{ secrets.NSCALE_HEADER_NAME }}
          NSCALE_API_KEY: ${{ secrets.NSCALE_API_KEY }}

      - name: Run Jules diagnostic
        id: jules-diag
        continue-on-error: true
        run: |
          npx ts-node scripts/oracle/diagnostic-jules.ts \
            --service-url "${{ secrets.JULES_BASE_URL }}" \
            --api-key "${{ secrets.JULES_API_KEY }}" \
            --timeout 30000

      - name: Run Mem0 diagnostic
        id: mem0-diag
        continue-on-error: true
        run: |
          npx ts-node scripts/oracle/diagnostic-mem0.ts \
            --service-url "${{ secrets.MCP_SERVER_URL }}" \
            --timeout 30000

      - name: Analyze performance metrics
        run: |
          echo "## üìä Performance Metrics" >> "$GITHUB_STEP_SUMMARY"
          echo "- DeepSeek P95 latency: $(cat deepseek-metrics.json | jq '.p95_latency_ms')" >> "$GITHUB_STEP_SUMMARY"
          echo "- Jules error rate: $(cat jules-metrics.json | jq '.error_rate')" >> "$GITHUB_STEP_SUMMARY"
          echo "- Mem0 write throughput: $(cat mem0-metrics.json | jq '.write_ops_per_sec')" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload diagnostic results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: oracle-diagnostics-${{ github.run_id }}
          path: |
            deepseek-metrics.json
            jules-metrics.json
            mem0-metrics.json
          retention-days: 30

  # ========================================
  # PHASE 3: Rate Limit Monitoring
  # ========================================
  rate-limit-check:
    name: Rate Limit & Quota Monitoring
    runs-on: ubuntu-latest
    needs: quick-health-check
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Check DeepSeek rate limits
        id: deepseek-ratelimit
        continue-on-error: true
        run: |
          RESPONSE=$(curl -s -i \
            -H "Authorization: Bearer ${{ secrets.DEEPSEEK_API_KEY }}" \
            "${{ secrets.DEEPSEEK_BASE_URL }}/v1/models" 2>&1)
          
          # Extract rate limit headers
          LIMIT=$(echo "$RESPONSE" | grep -i "x-ratelimit-limit" | cut -d' ' -f2)
          REMAINING=$(echo "$RESPONSE" | grep -i "x-ratelimit-remaining" | cut -d' ' -f2)
          RESET=$(echo "$RESPONSE" | grep -i "x-ratelimit-reset" | cut -d' ' -f2)
          
          echo "limit=$LIMIT" >> "$GITHUB_OUTPUT"
          echo "remaining=$REMAINING" >> "$GITHUB_OUTPUT"
          echo "reset=$RESET" >> "$GITHUB_OUTPUT"
          
          if [ -n "$REMAINING" ] && [ "$REMAINING" -lt 10 ]; then
            echo "‚ö†Ô∏è DeepSeek: Low quota ($REMAINING/$LIMIT remaining)"
          fi

      - name: Check Jules rate limits
        id: jules-ratelimit
        continue-on-error: true
        run: |
          RESPONSE=$(curl -s -i \
            -H "Authorization: Bearer ${{ secrets.JULES_API_KEY }}" \
            "${{ secrets.JULES_BASE_URL }}/health" 2>&1)
          
          REMAINING=$(echo "$RESPONSE" | grep -i "x-ratelimit-remaining" | cut -d' ' -f2)
          
          echo "remaining=$REMAINING" >> "$GITHUB_OUTPUT"
          
          if [ -n "$REMAINING" ] && [ "$REMAINING" -lt 5 ]; then
            echo "‚ö†Ô∏è Jules: Low quota ($REMAINING remaining)"
          fi

      - name: Alert if rate limits critical
        if: |
          (steps.deepseek-ratelimit.outputs.remaining < 5) ||
          (steps.jules-ratelimit.outputs.remaining < 2)
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: 'üö® Oracle Rate Limit Alert',
              attachments: [{
                color: 'danger',
                text: 'DeepSeek: ${{ steps.deepseek-ratelimit.outputs.remaining }} remaining\nJules: ${{ steps.jules-ratelimit.outputs.remaining }} remaining'
              }]
            }
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

  # ========================================
  # PHASE 4: Error Analysis
  # ========================================
  error-analysis:
    name: Error Rate & Pattern Analysis
    runs-on: ubuntu-latest
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Query workflow run data
        id: query
        run: |
          # Get recent workflow runs
          RUNS=$(gh run list \
            --workflow auto-reply.yml \
            --status completed \
            --limit 50 \
            --json conclusion,durationMinutes,url \
            2>/dev/null)
          
          # Count failures
          FAILURES=$(echo "$RUNS" | jq '[.[] | select(.conclusion=="failure")] | length')
          SUCCESSES=$(echo "$RUNS" | jq '[.[] | select(.conclusion=="success")] | length')
          
          if [ -n "$FAILURES" ] && [ -n "$SUCCESSES" ]; then
            ERROR_RATE=$(echo "scale=2; $FAILURES * 100 / ($FAILURES + $SUCCESSES)" | bc)
          else
            ERROR_RATE=0
          fi
          
          echo "error_rate=$ERROR_RATE" >> "$GITHUB_OUTPUT"
          echo "failures=$FAILURES" >> "$GITHUB_OUTPUT"
          echo "successes=$SUCCESSES" >> "$GITHUB_OUTPUT"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Check for error patterns
        run: |
          # Analyze recent logs for common error patterns
          echo "## üìâ Error Analysis" >> "$GITHUB_STEP_SUMMARY"
          echo "- Error rate (last 50 runs): ${{ steps.query.outputs.error_rate }}%" >> "$GITHUB_STEP_SUMMARY"
          echo "- Failures: ${{ steps.query.outputs.failures }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- Successes: ${{ steps.query.outputs.successes }}" >> "$GITHUB_STEP_SUMMARY"

      - name: Alert if error rate high
        if: ${{ steps.query.outputs.error_rate > 10 }}
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: '‚ö†Ô∏è Oracle High Error Rate',
              attachments: [{
                color: 'warning',
                text: 'Error rate: ${{ steps.query.outputs.error_rate }}% (threshold: 10%)'
              }]
            }
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

  # ========================================
  # PHASE 5: Daily Report Generation
  # ========================================
  daily-report:
    name: Generate Daily Report
    runs-on: ubuntu-latest
    needs: [quick-health-check, error-analysis]
    if: |
      github.event_name == 'schedule' &&
      github.event.schedule == '0 6 * * *'
    
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: pnpm

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Generate report
        id: report
        run: |
          npx ts-node scripts/oracle/generate-report.ts \
            --date "$(date -u +'%Y-%m-%d')" \
            --output oracle-report.md

      - name: Upload report to artifacts
        uses: actions/upload-artifact@v4
        with:
          name: oracle-daily-report-${{ github.run_number }}
          path: oracle-report.md
          retention-days: ${{ env.METRICS_RETENTION_DAYS }}

      - name: Create GitHub issue with report
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('oracle-report.md', 'utf8');
            
            const date = new Date().toISOString().split('T')[0];
            const timestamp = new Date().toISOString();
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üìä Oracle Daily Report ‚Äî ${date}`,
              body: `${report}\n\n---\n_Generated: ${timestamp}_`,
              labels: ['oracle-report', 'monitoring'],
              assignees: []
            });

      - name: Send report to Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: 'üìä Oracle Daily Report',
              attachments: [{
                color: 'good',
                text: 'See GitHub Issues for full report #oracle-report'
              }]
            }
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

  # ========================================
  # PHASE 6: Auto-Remediation
  # ========================================
  auto-remediation:
    name: Auto-Remediation on Failures
    runs-on: ubuntu-latest
    needs: quick-health-check
    if: needs.quick-health-check.outputs.all_healthy == 'false'

    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 10
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: pnpm

      - name: Attempt circuit breaker reset
        id: reset
        continue-on-error: true
        run: |
          npx ts-node scripts/oracle/reset-circuit-breakers.ts \
            --timeout 15000

      - name: Restart failed service containers (if applicable)
        continue-on-error: true
        run: |
          # This would depend on your infrastructure
          # Example: restart Docker containers, trigger Kubernetes rolling update, etc.
          echo "‚ÑπÔ∏è Service restart not automated (manual intervention required)"

      - name: Create incident issue
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'üö® Oracle System Alert: Services Degraded',
              body: `
## Issue
Oracle monitoring detected service degradation:
- DeepSeek: ${{ needs.quick-health-check.outputs.deepseek_status }}
- Jules: ${{ needs.quick-health-check.outputs.jules_status }}
- Mem0: ${{ needs.quick-health-check.outputs.mem0_status }}

## Timestamp
${{ needs.quick-health-check.outputs.check_timestamp }}

## Action Required
1. Check service health dashboards
2. Review recent deployments
3. Investigate error logs
4. Escalate if needed

See workflow logs for details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
              `,
              labels: ['oracle-alert', 'incident', 'priority-high'],
              assignees: []
            });

      - name: Notify team via Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: 'üö® Oracle System Alert',
              attachments: [{
                color: 'danger',
                text: 'Services degraded. Check GitHub issue #oracle-alert'
              }]
            }
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

# ========================================
# PHASE 7: Status Reporting
# ========================================
  final-status:
    name: Final Status Report
    runs-on: ubuntu-latest
    needs: [quick-health-check, error-analysis]
    if: always()

    steps:
      - name: Set workflow status badge
        uses: actions/github-script@v7
        with:
          script: |
            const isHealthy = '${{ needs.quick-health-check.outputs.all_healthy }}' === 'true';
            const status = isHealthy ? 'passing' : 'failing';
            const timestamp = '${{ needs.quick-health-check.outputs.check_timestamp }}';
            
            console.log(`Oracle Status: ${status} (${timestamp})`);

      - name: Post status to GitHub Discussions (optional)
        if: always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            // Post to discussions if enabled
            // This is optional; requires discussions enabled on repo
            console.log('Status posted to monitoring systems');

      - name: Set commit status
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const isHealthy = '${{ needs.quick-health-check.outputs.all_healthy }}' === 'true';
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: isHealthy ? 'success' : 'failure',
              description: 'Oracle system health check',
              context: 'Oracle/Health'
            });

      - name: Print summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # Oracle Health Check Summary
          
          | Component | Status | Response Time |
          |-----------|--------|--------|
          | DeepSeek | ${{ needs.quick-health-check.outputs.deepseek_status }} | ${{ needs.quick-health-check.outputs.deepseek_time_ms }}ms |
          | Jules | ${{ needs.quick-health-check.outputs.jules_status }} | ${{ needs.quick-health-check.outputs.jules_time_ms }}ms |
          | Mem0 | ${{ needs.quick-health-check.outputs.mem0_status }} | ${{ needs.quick-health-check.outputs.mem0_time_ms }}ms |
          
          **Overall Status:** ${{ needs.quick-health-check.outputs.all_healthy == 'true' && '‚úÖ Healthy' || '‚ö†Ô∏è Degraded' }}
          
          **Timestamp:** ${{ needs.quick-health-check.outputs.check_timestamp }}
          
          **Error Rate:** ${{ needs.error-analysis.outputs.error_rate }}%
          EOF

---

## Configuration

### Secrets Required
Add these GitHub secrets:
- `DEEPSEEK_API_KEY` ‚Äî DeepSeek API authentication
- `DEEPSEEK_BASE_URL` ‚Äî Custom DeepSeek endpoint
- `NSCALE_HEADER_NAME` ‚Äî NSCALE header name
- `NSCALE_API_KEY` ‚Äî NSCALE authentication
- `JULES_API_KEY` ‚Äî Jules event routing authentication
- `JULES_BASE_URL` ‚Äî Jules service endpoint
- `MCP_SERVER_URL` ‚Äî Mem0 Model Context Protocol server URL
- `SLACK_WEBHOOK_URL` ‚Äî Slack notifications (optional)

### Environment Variables
- `HEALTH_CHECK_TIMEOUT_MS` ‚Äî Timeout for health checks (default: 10000ms)
- `METRICS_RETENTION_DAYS` ‚Äî How long to keep metrics (default: 30)

### Scheduling
- **Hourly:** Quick health checks at top of each hour
- **Daily:** Full reports at 06:00 UTC
- **Manual:** Can be triggered via `workflow_dispatch`

---

## Alert Thresholds

| Metric | Threshold | Action |
|--------|-----------|--------|
| Service down | Any service unreachable | Create incident issue + Slack alert |
| Response time | >5s | Log warning |
| Error rate | >10% | Slack alert + incident issue |
| Rate limit | <5 requests remaining | Slack alert |
| Circuit breaker open | Any service | Attempt reset + create issue |

---

## Integration with Other Workflows

This monitoring workflow integrates with:
- `auto-reply.yml` ‚Äî Monitors DeepSeek calls
- `conflict-resolver.yml` ‚Äî Monitors DeepSeek + Jules
- `auto-improve.yml` ‚Äî Monitors DeepSeek + Jules
- `ci.yml` ‚Äî Monitors Jules event routing

---

## Troubleshooting

### Issue: Health checks timing out
**Fix:** Increase `HEALTH_CHECK_TIMEOUT_MS` in workflow

### Issue: Rate limit alerts constantly firing
**Fix:** Adjust `RATE_LIMIT_CHECK_THRESHOLD` in workflow

### Issue: Slack alerts not sending
**Fix:** Verify `SLACK_WEBHOOK_URL` secret is set correctly

---

## Summary

This workflow provides **24/7 monitoring** of the Oracle AI system with:
- ‚úÖ Hourly health checks
- ‚úÖ Automatic error detection & alerting
- ‚úÖ Rate limit monitoring
- ‚úÖ Daily compliance reports
- ‚úÖ Auto-remediation attempts
- ‚úÖ Incident tracking

**Next Steps:**
1. Add all required secrets to GitHub
2. Test manually with `workflow_dispatch`
3. Monitor for 1 week
4. Adjust thresholds based on baselines
5. Document runbooks for alerts
